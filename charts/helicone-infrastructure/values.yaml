################################################################################
#
#                     HELICONE INFRASTRUCTURE
#
################################################################################

# Override the full name of the release
fullnameOverride: "infra"

# Global configuration
global: {}

# Cluster Autoscaler configuration
clusterAutoscaler:
  enabled: false # Disabled to avoid conflicts with existing installation
  image:
    tag: "v1.26.2" # Should match your Kubernetes version
  clusterName: "helicone-cluster" # Replace with your actual cluster name
  serviceAccount:
    roleArn: "" # ARN of the IAM role for cluster autoscaler (must be set for production)
  extraArgs:
    - "--scale-down-delay-after-add=10m"
    - "--scale-down-unneeded-time=10m"
    - "--max-node-provision-time=15m"
    - "--scan-interval=10s"

# Beyla configuration for eBPF observability
beyla:
  enabled: false
  image:
    repository: grafana/beyla
    tag: "v1.4.0"
    pullPolicy: IfNotPresent

  # Configuration for Beyla
  config:
    logLevel: info
    services:
      openPorts: "8080,3000,8000"
      executableName: ""
    routes:
      unmatched: "heuristic"
      patterns: []
    tracing:
      enabled: true
      sampler:
        name: "parentbased_traceidratio"
        arg: "0.1"
    metrics:
      interval: "30s"
      buckets: [0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10]

  # Security context for Beyla
  securityContext:
    privileged: false
    capabilities:
      sysAdmin: false

  # OpenTelemetry configuration
  otel:
    endpoint: "http://otel-collector.helicone-infrastructure.svc.cluster.local:4317"
    headers: ""
    protocol: "grpc"

  # Additional environment variables for Beyla
  extraEnvVars: {}

  # Resource limits for Beyla sidecar
  resources:
    requests:
      memory: "64Mi"
      cpu: "50m"
    limits:
      memory: "128Mi"
      cpu: "100m"

# Service Account configuration
serviceAccount:
  name: "beyla"
  annotations: {}

# Nginx Ingress Controller configuration
nginxIngressController:
  enabled: true
  namespace: helicone-infrastructure
  fullnameOverride: "nginx-controller"

  # Watch all namespaces for ingress resources
  watchIngressWithoutClass: true
  watchNamespaces: "" # Empty string means watch all namespaces

  # Controller configuration
  controller:
    image:
      repository: registry.k8s.io/ingress-nginx/controller
      tag: "v1.8.2"
      pullPolicy: IfNotPresent

    # Enable cross-namespace ingress support
    extraArgs:
      - "--watch-ingress-without-class=true"
      - "--ingress-class=nginx"
      - "--enable-ssl-passthrough"

    # Resource configuration
    resources:
      requests:
        cpu: 100m
        memory: 90Mi
      limits:
        cpu: 500m
        memory: 512Mi

    # Service configuration
    service:
      type: LoadBalancer
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-type: nlb
        service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
      ports:
        http: 80
        https: 443

    # Ingress class configuration
    ingressClass:
      name: nginx
      enabled: true
      default: true
      controllerValue: "k8s.io/ingress-nginx"

    # RBAC configuration for cross-namespace access
    rbac:
      create: true
      scope: true # Cluster-wide scope for all namespaces

    # Service account
    serviceAccount:
      create: true
      name: nginx-ingress-controller

# Prometheus configuration
prometheus:
  enabled: true
  # When prometheus is enabled, these subcomponents are configured
  image:
    tag: "v3.4.1"
  alertmanager:
    enabled: false
  kube-state-metrics:
    enabled: false
  prometheus-node-exporter:
    enabled: false
  prometheus-pushgateway:
    enabled: false

  global:
    ## How frequently to scrape targets by default
    scrape_interval: 15s
    scrape_timeout: 10s
    ## How frequently to evaluate rules
    evaluation_interval: 15s

  resources:
    limits:
      cpu: 1000m
      memory: 512Mi
    requests:
      cpu: 500m
      memory: 512Mi

  # https://github.com/prometheus-community/helm-charts/blob/ec4f325616989d93c204012c57199e98e84b8c87/charts/prometheus/values.yaml#L702
  hostNetwork: true

  server:
    fullnameOverride: "prometheus"
    replicaCount: 1
    persistentVolume:
      mountPath: "/var/lib/prometheus"
    extraFlags:
      - web.enable-lifecycle
      - web.enable-remote-write-receiver
      - web.enable-otlp-receiver
      - enable-feature=exemplar-storage

# Tempo configuration for distributed tracing
tempo:
  enabled: true
  fullnameOverride: "tempo"
  persistence:
    enabled: true
    size: "10Gi"
  serviceMonitor:
    enabled: true
  retention: "7d"

  # Enable distributed mode
  traces:
    otlp:
      grpc:
        enabled: true
      http:
        enabled: true

  # Configure the distributor to receive OTLP
  distributor:
    config:
      receivers:
        otlp:
          protocols:
            grpc:
              endpoint: 0.0.0.0:4317
            http:
              endpoint: 0.0.0.0:4318

# Loki configuration
# Loki configuration for log aggregation
loki:
  enabled: false
  fullnameOverride: "loki"
  singleBinary:
    replicas: 1
  image:
    repository: grafana/loki
    tag: "3.5.0"
    pullPolicy: IfNotPresent

  # Resource configuration
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 1Gi

  # Service configuration
  service:
    type: ClusterIP
    ports:
      http:
        port: 3100
        targetPort: 3100

  # Loki configuration
  config:
    auth_enabled: false
    server:
      http_listen_port: 3100
      grpc_listen_port: 9096
    common:
      path_prefix: /loki
      storage:
        filesystem:
          chunks_directory: /loki/chunks
          rules_directory: /loki/rules
      replication_factor: 1
      ring:
        instance_addr: 127.0.0.1
        kvstore:
          store: inmemory
    query_scheduler:
      max_outstanding_requests_per_tenant: 32768
    schema_config:
      configs:
        - from: 2020-05-15
          store: tsdb
          object_store: filesystem
          schema: v13
          index:
            prefix: index_
            period: 24h
    ruler:
      alertmanager_url: http://localhost:9093
    limits_config:
      reject_old_samples: true
      reject_old_samples_max_age: 168h
      allow_structured_metadata: true
    analytics:
      reporting_enabled: false
  # Zero out replica counts of other deployment modes
  backend:
    replicas: 0
  read:
    replicas: 0
  write:
    replicas: 0

  ingester:
    replicas: 0
  querier:
    replicas: 0
  queryFrontend:
    replicas: 0
  queryScheduler:
    replicas: 0
  distributor:
    replicas: 0
  compactor:
    replicas: 0
  indexGateway:
    replicas: 0
  bloomCompactor:
    replicas: 0
  bloomGateway:
    replicas: 0

# OpenTelemetry Collector subchart values
opentelemetry-collector:
  enabled: true
  mode: deployment
  replicaCount: 1
  command:
    name: "otelcol"
  image:
    repository: "otel/opentelemetry-collector"
    tag: "0.127.0"

  fullnameOverride: "otel-collector"

  # Override the default config with your custom configuration
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317

    exporters:
      otlp:
        endpoint: tempo.helicone-infrastructure.svc.cluster.local:4317
        tls:
          insecure: true
      # otlphttp:
      #   endpoint: http://loki.helicone-infrastructure.svc.cluster.local:3100/otlp
      #   tls:
      #     insecure: true
      prometheusremotewrite:
        endpoint: http://prometheus.helicone-infrastructure.svc.cluster.local:80/api/v1/write
        tls:
          insecure: true

    extensions:
      health_check:
        endpoint: ${env:MY_POD_IP}:13133

    processors:
      batch:
        timeout: 1s
        send_batch_size: 1024
      memory_limiter:
        check_interval: 5s
        limit_mib: 500
        spike_limit_mib: 100

    service:
      extensions:
        - health_check
      pipelines:
        traces:
          receivers:
            - otlp
          processors:
            - batch
          exporters:
            - otlp
        metrics:
          receivers:
            - otlp
          processors:
            - batch
          exporters:
            - prometheusremotewrite
        # logs:
        #   receivers:
        #     - otlp
        #   processors:
        #     - batch
        #   exporters:
        #     - otlphttp

  # Resource configuration
  resources:
    limits:
      cpu: 500m
      memory: 1024Mi
    requests:
      cpu: 250m
      memory: 256Mi

  # Enable GOMEMLIMIT based on memory limits
  useGOMEMLIMIT: true

  # Service configuration
  service:
    type: ClusterIP

  # Only expose the OTLP ports
  ports:
    otlp:
      enabled: true
      containerPort: 4317
      servicePort: 4317
      protocol: TCP
      appProtocol: grpc
    otlp-http:
      enabled: false
    jaeger-compact:
      enabled: false
    jaeger-thrift:
      enabled: false
    jaeger-grpc:
      enabled: false
    zipkin:
      enabled: false
    metrics:
      enabled: false
